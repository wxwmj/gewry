import asyncio
import aiohttp
import base64
import time
import os
import shutil
from urllib.parse import urlparse
from asyncio import Semaphore
from datetime import datetime, timezone, timedelta
import glob

# é…ç½®å¸¸é‡
MAX_DELAY = 5000
MAX_SAVE = 6666
NODES_PER_FILE = 666
SUB_FILE = os.path.join("source", "subs.txt")  # é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ source æ–‡ä»¶å¤¹
OUTPUT_FILE_PREFIX = "sub"
SUPPORTED_PROTOCOLS = ("vmess://", "ss://", "trojan://", "vless://", "hysteria://", "hysteria2://", "tuic://")

# å¤±è´¥é“¾æ¥ä¿å­˜è·¯å¾„
FAIL_FOLDER = "fail"
FAIL_FILE = os.path.join(FAIL_FOLDER, "fail.txt")


def ensure_fail_folder_exists():
    """ç¡®ä¿ fail æ–‡ä»¶å¤¹å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º"""
    if not os.path.exists(FAIL_FOLDER):
        os.makedirs(FAIL_FOLDER)
        print(f"ğŸ“‚ åˆ›å»ºæ–‡ä»¶å¤¹: {FAIL_FOLDER}")


def save_failed_links(failed_links):
    """å°†å¤±è´¥çš„é“¾æ¥ä¿å­˜åˆ° fail æ–‡ä»¶å¤¹ä¸­çš„ fail.txt"""
    ensure_fail_folder_exists()
    with open(FAIL_FILE, "a", encoding="utf-8") as f:
        for link in failed_links:
            f.write(f"# {link}\n")  # æ³¨é‡Šæ‰å¤±è´¥çš„é“¾æ¥


def remove_duplicates_and_save():
    """ä» subs.txt è¯»å–é“¾æ¥å¹¶å»é‡ï¼Œæ›´æ–°æ–‡ä»¶"""
    try:
        with open(SUB_FILE, "r", encoding="utf-8") as f:
            urls = [line.strip() for line in f if line.strip()]
    except FileNotFoundError:
        print(f"[é”™è¯¯] æœªæ‰¾åˆ°æ–‡ä»¶ {SUB_FILE}")
        return []

    # å»é‡å¹¶ä¿ç•™å”¯ä¸€çš„é“¾æ¥
    unique_urls = list(set(urls))  # ä½¿ç”¨ set å»é‡

    # å°†å»é‡åçš„é“¾æ¥é‡æ–°å†™å…¥ subs.txt
    with open(SUB_FILE, "w", encoding="utf-8") as f:
        for url in unique_urls:
            f.write(f"{url}\n")

    return unique_urls


def is_supported_node(url):
    """åˆ¤æ–­é“¾æ¥æ˜¯å¦ä¸ºæ”¯æŒçš„åè®®"""
    return url.startswith(SUPPORTED_PROTOCOLS)


def base64_decode_links(data):
    """è§£ç  Base64 æ ¼å¼çš„è®¢é˜…é“¾æ¥"""
    try:
        decoded = base64.b64decode(data).decode("utf-8")
        return [line.strip() for line in decoded.strip().splitlines() if is_supported_node(line)]
    except Exception:
        return [line.strip() for line in data.strip().splitlines() if is_supported_node(line)]


async def fetch_subscription(session, url):
    """æŠ“å–è®¢é˜…å†…å®¹"""
    try:
        async with session.get(url, timeout=5) as resp:
            raw = await resp.text()
            return base64_decode_links(raw)
    except Exception:
        print(f"[å¤±è´¥] æŠ“å–è®¢é˜…å¤±è´¥ï¼Œå·²æ³¨é‡Šè¯¥é“¾æ¥: {url}")
        # ä¿å­˜å¤±è´¥é“¾æ¥ï¼Œæ³¨é‡Šæ‰è¯¥é“¾æ¥å¹¶è®°å½•åˆ° fail æ–‡ä»¶å¤¹
        save_failed_links([url])
        return []


def extract_host_port(node_url):
    """æå–èŠ‚ç‚¹çš„ä¸»æœºå’Œç«¯å£"""
    try:
        parsed = urlparse(node_url)
        if parsed.hostname and parsed.port:
            port = int(parsed.port)
            if 0 < port < 65536:
                return f"{parsed.hostname}:{port}"
    except Exception:
        return None
    return None


async def tcp_ping(host, port, timeout=3):
    """æµ‹è¯•èŠ‚ç‚¹å»¶è¿Ÿ"""
    try:
        start = time.perf_counter()
        reader, writer = await asyncio.wait_for(asyncio.open_connection(host, port), timeout)
        end = time.perf_counter()
        writer.close()
        await writer.wait_closed()
        return int((end - start) * 1000)
    except Exception:
        return None


async def test_single_node(node):
    """æµ‹è¯•å•ä¸ªèŠ‚ç‚¹çš„å»¶è¿Ÿ"""
    try:
        parsed = urlparse(node)
        host, port = parsed.hostname, parsed.port
        if not host or not port:
            return None
        delay = await tcp_ping(host, port, timeout=3)
        if delay is None or delay > MAX_DELAY:
            return None
        return node, delay
    except Exception:
        return None


def print_progress(percent, success_count):
    """æ‰“å°èŠ‚ç‚¹æµ‹è¯•è¿›åº¦"""
    line = f"æµ‹è¯•èŠ‚ç‚¹è¿›åº¦: {percent:6.2f}% | æˆåŠŸ: {success_count}"
    max_len = 50
    padded_line = line + " " * (max_len - len(line))
    print("\r" + padded_line, end="", flush=True)


async def test_all_nodes(nodes):
    """æµ‹è¯•æ‰€æœ‰èŠ‚ç‚¹çš„å»¶è¿Ÿå¹¶è¿”å›å¯ç”¨èŠ‚ç‚¹"""
    total = len(nodes)
    success_count = 0
    done_count = 0
    results = []
    sem = Semaphore(32)
    last_print_percent = 0

    async def test_node(node):
        nonlocal success_count, done_count, last_print_percent
        async with sem:
            res = await test_single_node(node)
            if res is not None:
                results.append(res)
                success_count += 1
            done_count += 1
            percent = done_count / total * 100
            if percent - last_print_percent >= 5 or percent == 100:
                print_progress(percent, success_count)
                last_print_percent = percent

    tasks = [test_node(node) for node in nodes]
    await asyncio.gather(*tasks)
    print()
    results.sort(key=lambda x: x[1])
    return [node for node, delay in results[:MAX_SAVE]]


def get_beijing_time():
    """è·å–åŒ—äº¬æ—¶é—´"""
    tz = timezone(timedelta(hours=8))
    return datetime.now(tz)


def clear_output_folder():
    """æ¸…ç©ºè¾“å‡ºæ–‡ä»¶å¤¹"""
    folder = "output"
    if not os.path.exists(folder):
        os.makedirs(folder)
        print(f"ğŸ“‚ åˆ›å»ºæ–‡ä»¶å¤¹: {folder}")
        return

    # åˆ é™¤ output æ–‡ä»¶å¤¹å†…æ‰€æœ‰æ–‡ä»¶å’Œå­ç›®å½•
    for item in os.listdir(folder):
        path = os.path.join(folder, item)
        try:
            if os.path.isfile(path):
                os.remove(path)
                print(f"ğŸ—‘ï¸ åˆ é™¤æ—§æ–‡ä»¶ï¼š{path}")
            elif os.path.isdir(path):
                shutil.rmtree(path)
                print(f"ğŸ—‘ï¸ åˆ é™¤æ—§ç›®å½•ï¼š{path}")
        except Exception as e:
            print(f"[é”™è¯¯] åˆ é™¤ {path} å¤±è´¥: {e}")


def get_output_folder():
    """è·å–è¾“å‡ºæ–‡ä»¶å¤¹"""
    folder = "output"
    if not os.path.exists(folder):
        os.makedirs(folder)
        print(f"ğŸ“‚ åˆ›å»ºæ–‡ä»¶å¤¹: {folder}")
    else:
        print(f"ğŸ“‚ ä½¿ç”¨ç°æœ‰æ–‡ä»¶å¤¹: {folder}")
    return folder


async def save_nodes_to_file(nodes, file_index, folder):
    """ä¿å­˜èŠ‚ç‚¹åˆ°æ–‡ä»¶"""
    if len(nodes) >= 99:
        file_name = os.path.join(folder, f"{OUTPUT_FILE_PREFIX}{file_index}.txt")
        with open(file_name, "w", encoding="utf-8") as f:
            combined = "\n".join(nodes)
            encoded = base64.b64encode(combined.encode("utf-8")).decode("utf-8")
            f.write(encoded)
        print(f"ğŸ“¦ æ–‡ä»¶ {file_name} ä¿å­˜æˆåŠŸï¼ŒèŠ‚ç‚¹æ•°: {len(nodes)}")
    else:
        print(f"[è·³è¿‡] æ–‡ä»¶ {file_index} èŠ‚ç‚¹æ•°ä¸è¶³ 99ï¼Œä¸ä¿å­˜ã€‚")


async def main():
    print("ğŸ“¥ è¯»å–å¹¶å»é‡è®¢é˜…é“¾æ¥...")
    # è¯»å–å¹¶å»é‡åçš„è®¢é˜…é“¾æ¥
    urls = remove_duplicates_and_save()

    if not urls:
        print("[é”™è¯¯] æ²¡æœ‰æœ‰æ•ˆçš„è®¢é˜…é“¾æ¥å¯ç”¨")
        return

    print("ğŸŒ æŠ“å–è®¢é˜…å†…å®¹ä¸­...")
    async with aiohttp.ClientSession() as session:
        all_nodes = []
        failed_links = []

        for url in urls:
            nodes = await fetch_subscription(session, url)
            if nodes:
                print(f"[æˆåŠŸ] æŠ“å–è®¢é˜…ï¼š{url}ï¼ŒèŠ‚ç‚¹æ•°: {len(nodes)}")
                all_nodes.extend(nodes)
            else:
                failed_links.append(url)

        if failed_links:
            print(f"[æ³¨æ„] {len(failed_links)} ä¸ªè®¢é˜…é“¾æ¥æŠ“å–å¤±è´¥ï¼Œå·²ä¿å­˜è‡³ {FAIL_FILE}")

    # å»é‡åçš„èŠ‚ç‚¹å¤„ç†
    print(f"ğŸ“Š æŠ“å–å®Œæˆï¼ŒèŠ‚ç‚¹æ€»æ•°ï¼ˆå«é‡å¤ï¼‰: {len(all_nodes)}")
    unique_nodes_map = {extract_host_port(n): n for n in all_nodes if extract_host_port(n)}
    unique_nodes = list(unique_nodes_map.values())
    print(f"ğŸ¯ å»é‡åèŠ‚ç‚¹æ•°: {len(unique_nodes)}")

    print(f"ğŸš¦ å¼€å§‹èŠ‚ç‚¹å»¶è¿Ÿæµ‹è¯•ï¼Œå…± {len(unique_nodes)} ä¸ªèŠ‚ç‚¹")
    tested_nodes = await test_all_nodes(unique_nodes)
    print(f"\nâœ… æµ‹è¯•å®Œæˆ: æˆåŠŸ {len(tested_nodes)} / æ€» {len(unique_nodes)}")

    if not tested_nodes:
        print("[ç»“æœ] æ²¡æœ‰å¯ç”¨èŠ‚ç‚¹")
        return

    # è·å–è¾“å‡ºæ–‡ä»¶å¤¹
    output_folder = get_output_folder()

    # æ¸…ç©ºæ—§æ–‡ä»¶å¤¹
    clear_output_folder()

    # ä¿å­˜æœ‰æ•ˆèŠ‚ç‚¹
    print("ğŸ’¾ ä¿å­˜æœ‰æ•ˆèŠ‚ç‚¹...")
    output_file_count = 1
    for i in range(0, len(tested_nodes), NODES_PER_FILE):
        nodes_chunk = tested_nodes[i:i + NODES_PER_FILE]
        await save_nodes_to_file(nodes_chunk, output_file_count, output_folder)
        output_file_count += 1

if __name__ == "__main__":
    asyncio.run(main())
